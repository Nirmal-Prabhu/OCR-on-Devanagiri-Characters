{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing OCR for Hindi Letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the required files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal ML data analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# File manipulation\n",
    "import os\n",
    "\n",
    "# Image manipulation\n",
    "from skimage.io import imread\n",
    "\n",
    "# DL Training and evaluation\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChars(PATH):\n",
    "    chars = os.listdir(PATH)\n",
    "    new_chars = []\n",
    "    for char in chars:\n",
    "        new_chars.append(char)\n",
    "    return new_chars\n",
    "\n",
    "def getImages(PATH, categories):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in categories:\n",
    "        path = os.path.join(PATH,i) # Copying path of a specific image\n",
    "        for img in os.listdir(path):\n",
    "            img_array = imread(os.path.join(path,img))/255 # Taking input of image as an array\n",
    "            X.append(img_array) # Storing image in the form of array\n",
    "            y.append(categories[i]) # Storing its corresponding category\n",
    "    \n",
    "    # Converting to numpy array form    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Reshape X to pass into CNN without issues\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindir = '../data/DevanagariHandwrittenCharacterDataset/Train'\n",
    "testdir = '../data/DevanagariHandwrittenCharacterDataset/Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'character_10_yna': 0,\n",
       " 'character_11_taamatar': 1,\n",
       " 'character_12_thaa': 2,\n",
       " 'character_13_daa': 3,\n",
       " 'character_14_dhaa': 4,\n",
       " 'character_15_adna': 5,\n",
       " 'character_16_tabala': 6,\n",
       " 'character_17_tha': 7,\n",
       " 'character_18_da': 8,\n",
       " 'character_19_dha': 9,\n",
       " 'character_1_ka': 10,\n",
       " 'character_20_na': 11,\n",
       " 'character_21_pa': 12,\n",
       " 'character_22_pha': 13,\n",
       " 'character_23_ba': 14,\n",
       " 'character_24_bha': 15,\n",
       " 'character_25_ma': 16,\n",
       " 'character_26_yaw': 17,\n",
       " 'character_27_ra': 18,\n",
       " 'character_28_la': 19,\n",
       " 'character_29_waw': 20,\n",
       " 'character_2_kha': 21,\n",
       " 'character_30_motosaw': 22,\n",
       " 'character_31_petchiryakha': 23,\n",
       " 'character_32_patalosaw': 24,\n",
       " 'character_33_ha': 25,\n",
       " 'character_34_chhya': 26,\n",
       " 'character_35_tra': 27,\n",
       " 'character_36_gya': 28,\n",
       " 'character_3_ga': 29,\n",
       " 'character_4_gha': 30,\n",
       " 'character_5_kna': 31,\n",
       " 'character_6_cha': 32,\n",
       " 'character_7_chha': 33,\n",
       " 'character_8_ja': 34,\n",
       " 'character_9_jha': 35,\n",
       " 'digit_0': 36,\n",
       " 'digit_1': 37,\n",
       " 'digit_2': 38,\n",
       " 'digit_3': 39,\n",
       " 'digit_4': 40,\n",
       " 'digit_5': 41,\n",
       " 'digit_6': 42,\n",
       " 'digit_7': 43,\n",
       " 'digit_8': 44,\n",
       " 'digit_9': 45}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = getChars(traindir)\n",
    "categories = {k: v for v, k in enumerate(characters)}\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = getImages(traindir, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78200, 32, 32, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78200,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = getImages(testdir, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13800, 32, 32, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 45, 45, 45])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(32,32,1)))\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(len(characters)))\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer_fn = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer_fn,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1222/1222 [==============================] - 4s 2ms/step - loss: 2.9322 - accuracy: 0.2034\n",
      "Epoch 2/10\n",
      "1222/1222 [==============================] - 3s 3ms/step - loss: 2.4108 - accuracy: 0.3179\n",
      "Epoch 3/10\n",
      "1222/1222 [==============================] - 3s 2ms/step - loss: 2.2853 - accuracy: 0.3491\n",
      "Epoch 4/10\n",
      "1222/1222 [==============================] - 3s 2ms/step - loss: 2.2041 - accuracy: 0.3694\n",
      "Epoch 5/10\n",
      "1222/1222 [==============================] - 3s 3ms/step - loss: 2.1677 - accuracy: 0.3798\n",
      "Epoch 6/10\n",
      "1222/1222 [==============================] - 3s 3ms/step - loss: 2.1364 - accuracy: 0.3873\n",
      "Epoch 7/10\n",
      "1222/1222 [==============================] - 3s 3ms/step - loss: 2.1171 - accuracy: 0.3973\n",
      "Epoch 8/10\n",
      "1222/1222 [==============================] - 3s 2ms/step - loss: 2.0956 - accuracy: 0.3983\n",
      "Epoch 9/10\n",
      "1222/1222 [==============================] - 3s 2ms/step - loss: 2.0807 - accuracy: 0.4049\n",
      "Epoch 10/10\n",
      "1222/1222 [==============================] - 3s 2ms/step - loss: 2.0687 - accuracy: 0.4071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2556e429250>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - 1s 2ms/step - loss: 1.5017 - accuracy: 0.6065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5016812086105347, 0.606521725654602]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the accuracy of the Neural Network isn't that impressive. The simple reason is that we have flattened our image to pass it in. Thus, a lot of the useful information and interdependence is overlooked. To avoid that, we can use a Convolutional Neural Network (CNN) which is proven to be one of the best in image-related NN models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 1), padding='same'))\n",
    "model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((4,4)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(32))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(len(characters)))\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer_fn = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer_fn,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1222/1222 [==============================] - 12s 7ms/step - loss: 0.7815 - accuracy: 0.7772\n",
      "Epoch 2/10\n",
      "1222/1222 [==============================] - 9s 7ms/step - loss: 0.2840 - accuracy: 0.9145\n",
      "Epoch 3/10\n",
      "1222/1222 [==============================] - 9s 7ms/step - loss: 0.2115 - accuracy: 0.9359\n",
      "Epoch 4/10\n",
      "1222/1222 [==============================] - 9s 7ms/step - loss: 0.1785 - accuracy: 0.9443\n",
      "Epoch 5/10\n",
      "1222/1222 [==============================] - 9s 7ms/step - loss: 0.1601 - accuracy: 0.9504\n",
      "Epoch 6/10\n",
      "1222/1222 [==============================] - 9s 7ms/step - loss: 0.1425 - accuracy: 0.9555\n",
      "Epoch 7/10\n",
      "1222/1222 [==============================] - 9s 7ms/step - loss: 0.1337 - accuracy: 0.9579\n",
      "Epoch 8/10\n",
      "1222/1222 [==============================] - 9s 7ms/step - loss: 0.1251 - accuracy: 0.9599\n",
      "Epoch 9/10\n",
      "1222/1222 [==============================] - 9s 7ms/step - loss: 0.1175 - accuracy: 0.9624\n",
      "Epoch 10/10\n",
      "1222/1222 [==============================] - 9s 7ms/step - loss: 0.1111 - accuracy: 0.9642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2557f9a7040>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - 1s 2ms/step - loss: 0.1061 - accuracy: 0.9713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10612078011035919, 0.9713043570518494]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 40+ class classification and our model is more than 95% accurate. This shows the effectivensess of CNNs in general and in this case, its effectiveness in learning character objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the CNN model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../models/NN_model.h5'\n",
    "model.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking with a separate digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPic = '../data/test.png'\n",
    "img_array = imread(myPic)/255 # Taking input of image as an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image loaded is the hindi digit 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x256244e5c70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_array, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdf = np.array([img_array])\n",
    "imdf = np.reshape(imdf, (imdf.shape[0], imdf.shape[1], imdf.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename = '../models/NN_model.h5'\n",
    "loaded_model = tf.keras.models.load_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "prd = list(loaded_model.predict(imdf)[0])\n",
    "print(prd.index(max(prd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'digit_3'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_key(dictionary, val):\n",
    "    for key, value in dictionary.items():\n",
    "         if val == value:\n",
    "                return key\n",
    "    return None\n",
    "\n",
    "get_key(categories, prd.index(max(prd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is able to predict the number accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
